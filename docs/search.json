[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nJenny Li\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nJenny Li\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "jensite",
    "section": "",
    "text": "import pandas as pd\nkarlan_list_2007 = pd.read_stata(\"~/Desktop/MSBA/SP/MGTA495b/hw1/karlan_list_2007.dta\")\nkarlan_list_2007.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nkarlan_list_2007.describe(include='all')\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nunique\nNaN\nNaN\n4\nNaN\nNaN\n5\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\nNaN\nControl\nNaN\nNaN\nControl\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\nNaN\n16687\nNaN\nNaN\n16687\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n0.666813\n0.333187\nNaN\n0.222311\n0.222211\nNaN\n0.166723\n0.166623\n0.166723\n0.166743\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\nNaN\n0.415803\n0.415736\nNaN\n0.372732\n0.372643\n0.372732\n0.372750\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n11 rows × 51 columns\n\n\n\n\nkarlan_list_2007.hist(figsize=(20, 20), bins=30, edgecolor='black')\n\narray([[&lt;Axes: title={'center': 'treatment'}&gt;,\n        &lt;Axes: title={'center': 'control'}&gt;,\n        &lt;Axes: title={'center': 'ratio2'}&gt;,\n        &lt;Axes: title={'center': 'ratio3'}&gt;,\n        &lt;Axes: title={'center': 'size25'}&gt;,\n        &lt;Axes: title={'center': 'size50'}&gt;,\n        &lt;Axes: title={'center': 'size100'}&gt;],\n       [&lt;Axes: title={'center': 'sizeno'}&gt;,\n        &lt;Axes: title={'center': 'askd1'}&gt;,\n        &lt;Axes: title={'center': 'askd2'}&gt;,\n        &lt;Axes: title={'center': 'askd3'}&gt;,\n        &lt;Axes: title={'center': 'ask1'}&gt;,\n        &lt;Axes: title={'center': 'ask2'}&gt;,\n        &lt;Axes: title={'center': 'ask3'}&gt;],\n       [&lt;Axes: title={'center': 'amount'}&gt;,\n        &lt;Axes: title={'center': 'gave'}&gt;,\n        &lt;Axes: title={'center': 'amountchange'}&gt;,\n        &lt;Axes: title={'center': 'hpa'}&gt;,\n        &lt;Axes: title={'center': 'ltmedmra'}&gt;,\n        &lt;Axes: title={'center': 'freq'}&gt;,\n        &lt;Axes: title={'center': 'years'}&gt;],\n       [&lt;Axes: title={'center': 'year5'}&gt;,\n        &lt;Axes: title={'center': 'mrm2'}&gt;,\n        &lt;Axes: title={'center': 'dormant'}&gt;,\n        &lt;Axes: title={'center': 'female'}&gt;,\n        &lt;Axes: title={'center': 'couple'}&gt;,\n        &lt;Axes: title={'center': 'state50one'}&gt;,\n        &lt;Axes: title={'center': 'nonlit'}&gt;],\n       [&lt;Axes: title={'center': 'cases'}&gt;,\n        &lt;Axes: title={'center': 'statecnt'}&gt;,\n        &lt;Axes: title={'center': 'stateresponse'}&gt;,\n        &lt;Axes: title={'center': 'stateresponset'}&gt;,\n        &lt;Axes: title={'center': 'stateresponsec'}&gt;,\n        &lt;Axes: title={'center': 'stateresponsetminc'}&gt;,\n        &lt;Axes: title={'center': 'perbush'}&gt;],\n       [&lt;Axes: title={'center': 'close25'}&gt;,\n        &lt;Axes: title={'center': 'red0'}&gt;,\n        &lt;Axes: title={'center': 'blue0'}&gt;,\n        &lt;Axes: title={'center': 'redcty'}&gt;,\n        &lt;Axes: title={'center': 'bluecty'}&gt;,\n        &lt;Axes: title={'center': 'pwhite'}&gt;,\n        &lt;Axes: title={'center': 'pblack'}&gt;],\n       [&lt;Axes: title={'center': 'page18_39'}&gt;,\n        &lt;Axes: title={'center': 'ave_hh_sz'}&gt;,\n        &lt;Axes: title={'center': 'median_hhincome'}&gt;,\n        &lt;Axes: title={'center': 'powner'}&gt;,\n        &lt;Axes: title={'center': 'psch_atlstba'}&gt;,\n        &lt;Axes: title={'center': 'pop_propurban'}&gt;, &lt;Axes: &gt;]],\n      dtype=object)\n\n\n\n\n\n\n\n\n\n\nfemale – Gender may influence donation behavior and responsiveness to messaging, making it important to ensure it’s balanced across groups.\ncouple – Whether someone donates as part of a couple could affect donation patterns, so this household structure variable should be checked for randomization balance.\nyears – The number of years since the initial donation reflects donor loyalty and long-term engagement, which could correlate with future giving behavior.\nfreq – The frequency of past donations is a proxy for prior engagement and generosity, which could confound treatment effects if unbalanced.\nave_hh_sz – Average household size within a zip code captures demographic background and socioeconomic context, which may indirectly influence charitable behavior.\n\n\nBalance Test\n\ntodo\ntest a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\n\nimport numpy as np\nfrom scipy import stats\nselected_vars = ['female', 'couple', 'years', 'freq', 'ave_hh_sz']\n\ndef manual_t_test(var):\n    data = karlan_list_2007[[var, 'treatment']].dropna()\n    group_A = data[data['treatment'] == 1][var]  # Treatment group\n    group_B = data[data['treatment'] == 0][var]  # Control group\n    X_A = group_A.mean()\n    X_B = group_B.mean()\n    S_A2 = group_A.var(ddof=1)\n    S_B2 = group_B.var(ddof=1)\n    N_A = len(group_A)\n    N_B = len(group_B)\n    se = np.sqrt(S_A2 / N_A + S_B2 / N_B)\n    t_stat = (X_A - X_B) / se\n    df_approx = min(N_A, N_B) - 1\n    p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df_approx))\n\n    return {\n        \"variable\": var,\n        \"mean_treatment\": X_A,\n        \"mean_control\": X_B,\n        \"t_stat\": t_stat,\n        \"p_value\": p_value,\n        \"significant_95\": p_value &lt; 0.05\n    }\n\nmanual_t_results = [manual_t_test(var) for var in selected_vars]\nfor result in manual_t_results:\n    result[\"coef\"] = result[\"mean_treatment\"] - result[\"mean_control\"]\n\nmanual_t_karlan_list_2007 = pd.DataFrame(manual_t_results)\nprint(manual_t_karlan_list_2007)\n\n    variable  mean_treatment  mean_control    t_stat   p_value  \\\n0     female        0.275151      0.282698 -1.753513  0.079533   \n1     couple        0.091358      0.092975 -0.582258  0.560401   \n2      years        6.078365      6.135914 -1.090918  0.275325   \n3       freq        8.035364      8.047342 -0.110845  0.911741   \n4  ave_hh_sz        2.430015      2.427002  0.823351  0.410321   \n\n   significant_95      coef  \n0           False -0.007547  \n1           False -0.001617  \n2           False -0.057549  \n3           False -0.011979  \n4           False  0.003012  \n\n\nThe results of the t-tests for five pre-treatment variables—female, couple, years, freq, and ave_hh_sz—indicate that there are no statistically significant differences between the treatment and control groups at the 95% confidence level. This suggests that the randomization process was successful in creating comparable groups across these covariates. While the variable female shows a p-value of 0.0795, which is marginally significant at the 10% level, it still does not meet the stricter 5% threshold. The other variables, including couple (p = 0.56), years since first donation (p = 0.275), frequency of past donations (freq, p = 0.912), and average household size (ave_hh_sz, p = 0.410), all have p-values well above 0.05, reinforcing the conclusion that the treatment and control groups are well balanced. This balance is crucial, as it supports the validity of causal inference by ensuring that any observed differences in outcomes are likely due to the treatment rather than pre-existing differences.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Container for regression results\nregression_results = []\n\nfor var in selected_vars:\n    formula = f\"{var} ~ treatment\"\n    model = smf.ols(formula, data=karlan_list_2007).fit()\n\n    coef = model.params['treatment']\n    std_err = model.bse['treatment']\n    p_val = model.pvalues['treatment']\n\n    regression_results.append({\n        \"Variable\": var,\n        \"Coefficient (β1)\": round(coef, 4),\n        \"Std. Error\": round(std_err, 4),\n        \"p-value\": round(p_val, 4)\n    })\n\n# Display as DataFrame\nregression_table = pd.DataFrame(regression_results)\nregression_table.set_index(\"Variable\", inplace=True)\nregression_table\n\n\n\n\n\n\n\n\nCoefficient (β1)\nStd. Error\np-value\n\n\nVariable\n\n\n\n\n\n\n\nfemale\n-0.0075\n0.0043\n0.0787\n\n\ncouple\n-0.0016\n0.0028\n0.5594\n\n\nyears\n-0.0575\n0.0522\n0.2700\n\n\nfreq\n-0.0120\n0.1080\n0.9117\n\n\nave_hh_sz\n0.0030\n0.0037\n0.4098\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nseparate_regression_results = []\n\nfor var in selected_vars:\n    X_var = sm.add_constant(karlan_list_2007[[var]])\n    y = karlan_list_2007['treatment']\n\n    model_var = sm.OLS(y, X_var, missing='drop').fit()\n    \n    separate_regression_results.append({\n        \"variable\": var,\n        \"coef\": model_var.params[var],\n        \"p_value\": model_var.pvalues[var],\n        \"r_squared\": model_var.rsquared\n    })\n\nseparate_regression_df = pd.DataFrame(separate_regression_results)\n\nprint(separate_regression_df)\n\n    variable      coef   p_value     r_squared\n0     female -0.008366  0.078691  6.313390e-05\n1     couple -0.004308  0.559365  6.964784e-06\n2      years -0.000422  0.270016  2.429441e-05\n3       freq -0.000020  0.911702  2.455472e-07\n4  ave_hh_sz  0.004678  0.409801  1.408946e-05\n\n\n\n\n\nCharitable Contribution Made\n\ntodo:\nmake a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control._\n\nimport matplotlib.pyplot as plt\n\n# Calculate proportions\ntreatment_proportion = karlan_list_2007[karlan_list_2007['treatment'] == 1]['gave'].mean()\ncontrol_proportion = karlan_list_2007[karlan_list_2007['treatment'] == 0]['gave'].mean()\n\n# Create the bar plot\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['blue', 'orange'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\nplt.ylim(0, 0.025)\nplt.show()\n\n\n\n\n\n\n\n\n\n\ntodo:\nrun a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)_\n\n# T-test for the binary outcome 'gave'\ngave_treatment = karlan_list_2007[karlan_list_2007['treatment'] == 1]['gave']\ngave_control = karlan_list_2007[karlan_list_2007['treatment'] == 0]['gave']\n\nresult = manual_t_test('gave')\nresult[\"coef\"] = result[\"mean_treatment\"] - result[\"mean_control\"]\nprint(pd.DataFrame([result]))\n\n  variable  mean_treatment  mean_control    t_stat   p_value  significant_95  \\\n0     gave        0.022039      0.017858  3.209462  0.001332            True   \n\n      coef  \n0  0.00418  \n\n\n\n# Bivariate linear regression for 'gave' ~ 'treatment'\nimport statsmodels.api as sm\n\nX = sm.add_constant(karlan_list_2007['treatment'])  \ny = karlan_list_2007['gave']\n\nmodel = sm.OLS(y, X, missing='drop').fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        15:41:16   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe results from both the t-test and linear regression indicate a statistically significant difference in donation behavior between the treatment and control groups. Specifically, individuals who received the treatment—meaning they were told about the matching grant—were significantly more likely to donate compared to those in the control group, who received no such incentive.\nThis finding suggests that even a simple message about a matching donation can meaningfully increase the likelihood of giving. It highlights an important aspect of human behavior: people are more likely to contribute to a cause when they believe their donation will have amplified impact. The idea that someone else is willing to “match” their gift seems to motivate action—perhaps because it signals legitimacy, urgency, or enhanced value of their contribution.\nIn practical terms, this supports the effectiveness of matching grants as a behavioral nudge in charitable fundraising. Organizations seeking to increase donor participation can likely benefit from this strategy, as it taps into psychological mechanisms like reciprocity, perceived value, and social proof.\n\n\ntodo: ??????\nrun a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._\n\nfrom statsmodels.discrete.discrete_model import Probit\n\n# karlan_list_2007_cleaned = karlan_list_2007[['gave', 'treatment']].dropna()\n\n# X = sm.add_constant(karlan_list_2007_cleaned['treatment'])\n# y = karlan_list_2007_cleaned['gave']\n\n# probit_model = sm.Probit(y, X).fit()\n\n# marginal_effects = probit_model.get_margeff(at='overall')\n# print(marginal_effects.summary())\n\nprobit_model = smf.probit('gave ~ treatment', data=karlan_list_2007).fit()\nmarginal_effects = probit_model.get_margeff(at='overall')\nprint(marginal_effects.summary())\n# print(probit_model.summary2().tables[1])\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\n\nDifferences between Match Rates\n\ntodo:\nUse a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?_\n\ndef test_match_ratio_effect(group_var_1, group_var_2, label1, label2):\n    # Create a temporary subset containing only the two comparison groups\n    subset = karlan_list_2007[(karlan_list_2007[group_var_1] == 1) | \n                              (karlan_list_2007[group_var_2] == 1)].copy()\n    \n    # Redefine 'treatment' column in this subset only\n    subset['treatment'] = subset[group_var_1]  # 1 if group_var_1, 0 if group_var_2\n\n    # Now call the manual_t_test function on the temporary data\n    data_backup = karlan_list_2007.copy()  # save original\n    globals()['karlan_list_2007'] = subset  # overwrite temporarily\n\n    result = manual_t_test('gave')\n    result['comparison'] = f\"{label1} vs {label2}\"\n\n    globals()['karlan_list_2007'] = data_backup  # restore original\n    return result\n\n# Run comparisons again\nresults = [\n    test_match_ratio_effect('ratio2', 'ratio', \"2:1\", \"1:1\"),\n    test_match_ratio_effect('ratio3', 'ratio', \"3:1\", \"1:1\"),\n    test_match_ratio_effect('ratio3', 'ratio2', \"3:1\", \"2:1\")\n]\n\n# Display results\nmatch_ratio_effects_df = pd.DataFrame(results)\nprint(match_ratio_effects_df[['comparison', 'mean_treatment', 'mean_control', 't_stat', 'p_value', 'significant_95']])\n\n   comparison  mean_treatment  mean_control    t_stat   p_value  \\\n0  2:1 vs 1:1        0.022633      0.020749  0.965049  0.334541   \n1  3:1 vs 1:1        0.022733      0.020749  1.015017  0.310120   \n2  3:1 vs 2:1        0.022733      0.022633  0.050116  0.960031   \n\n   significant_95  \n0           False  \n1           False  \n2           False  \n\n\nThe results from my t-tests align with the authors’ comment on page 8 of the paper, which states that neither the match threshold nor the match ratio appears to have a meaningful influence on donor behavior. Specifically, my analysis shows that the differences in donation rates between the 1:1, 2:1, and 3:1 match conditions are small and statistically insignificant. The p-values for all pairwise comparisons are well above the conventional 0.05 threshold, and the t-statistics are close to zero, indicating no meaningful difference in the likelihood of donating across these conditions. This supports the paper’s finding that while offering a match increases giving relative to no match, increasing the match ratio beyond 1:1 does not lead to additional gains. My findings reinforce the idea that it is the presence of a match—rather than its size—that primarily drives donor behavior.\n\n\ntodo:\nAssess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision._\n\n# Filter data to treatment group with valid ratio and non-missing 'gave'\nmatched_df = karlan_list_2007[\n    (karlan_list_2007['treatment'] == 1) &\n    (karlan_list_2007['ratio'].isin([1, 2, 3])) &\n    (karlan_list_2007['gave'].notnull())\n].copy()\n\n# Create dummy variables for each match ratio\nkarlan_list_2007['ratio1'] = (karlan_list_2007['ratio'] == 1).astype(int)\n\nmatched_df['ratio2'] = (matched_df['ratio'] == 2).astype(int)\nmatched_df['ratio3'] = (matched_df['ratio'] == 3).astype(int)\n\n# Run OLS regression: gave ~ ratio1 + ratio2 + ratio3\nX = sm.add_constant(matched_df[['ratio2', 'ratio3']])\ny = matched_df['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.524\nTime:                        16:36:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\ntodo:\nCalculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?_\n\n# Run regression: gave ~ ratio1 + ratio2 + ratio3\nmodel = smf.ols('gave ~ ratio1 + ratio2 + ratio3', data=karlan_list_2007).fit()\n\n# Compute actual response rates from data\nresponse_1to1 = karlan_list_2007[karlan_list_2007['ratio1'] == 1]['gave'].mean()\nresponse_2to1 = karlan_list_2007[karlan_list_2007['ratio2'] == 1]['gave'].mean()\nresponse_3to1 = karlan_list_2007[karlan_list_2007['ratio3'] == 1]['gave'].mean()\n\n# Calculate actual differences in response rates\ndiff_2v1_actual = response_2to1 - response_1to1\ndiff_3v2_actual = response_3to1 - response_2to1\n\n# Extract fitted coefficients\ncoef_ratio1 = model.params['ratio1']\ncoef_ratio2 = model.params['ratio2']\ncoef_ratio3 = model.params['ratio3']\n\n# Calculate differences in fitted coefficients\ndiff_2v1_reg = coef_ratio2 - coef_ratio1\ndiff_3v2_reg = coef_ratio3 - coef_ratio2\n\n# Display results\nprint(\"Actual Response Rate Differences:\")\nprint(f\"2:1 vs 1:1 = {diff_2v1_actual:.6f}\")\nprint(f\"3:1 vs 2:1 = {diff_3v2_actual:.6f}\")\n\nprint(\"\\nFitted Coefficient Differences:\")\nprint(f\"2:1 vs 1:1 = {diff_2v1_reg:.6f}\")\nprint(f\"3:1 vs 2:1 = {diff_3v2_reg:.6f}\")\n\nActual Response Rate Differences:\n2:1 vs 1:1 = 0.001884\n3:1 vs 2:1 = 0.000100\n\nFitted Coefficient Differences:\n2:1 vs 1:1 = 0.001884\n3:1 vs 2:1 = 0.000100\n\n\n\n\n\nSize of Charitable Contribution\n\ntodo:\nCalculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_\n\nresult = manual_t_test(\"amount\")\nprint(pd.DataFrame([result]))\n\n  variable  mean_treatment  mean_control    t_stat   p_value  significant_95\n0   amount        0.966873      0.813268  1.918228  0.055099           False\n\n\nThis t-test shows that the average donation amount was higher in the treatment group, but the difference is not quite statistically significant at the 5% level. The p-value of 0.055 is just above the 0.05 threshold, suggesting that the evidence for a treatment effect on donation amount is suggestive, but not conclusive.\nThis tells us that being offered a match may increase donation amounts, but the effect is relatively modest and not strong enough to be certain under typical statistical standards.\n\n\ntodo:\nnow limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?_\n\ndonors_only = karlan_list_2007[karlan_list_2007['amount'] &gt; 0]\ndonors_model = smf.ols('amount ~ treatment', data=donors_only).fit()\nprint(donors_model.summary2().tables[1])\n\n               Coef.  Std.Err.          t         P&gt;|t|     [0.025     0.975]\nIntercept  45.540268  2.423378  18.792063  5.473578e-68  40.784958  50.295579\ntreatment  -1.668393  2.872384  -0.580839  5.614756e-01  -7.304773   3.967986\n\n\nThis analysis shows that, conditional on donating, individuals in the treatment group did not give more than those in the control group — in fact, they gave slightly less, though the result is not statistically meaningful. In short: while the treatment may increase the likelihood of giving, it does not significantly affect how much is given once someone decides to donate. It seems that the treatment coefficient does not have a causal interpretation in this context. The sample is now restricted to those who chose to give, which introduces selection bias. This means we are no longer comparing randomly assigned groups, and the treatment effect can’t be causally interpreted. This regression only tells us something about donor behavior, not about the overall treatment effect on donation amount.\n\n\ntodo:\nMake two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot._\n\n# Split by treatment status\ntreatment_donors = donors_only[donors_only['treatment'] == 1]['amount']\ncontrol_donors = donors_only[donors_only['treatment'] == 0]['amount']\n\n# Compute means\nmean_treatment = treatment_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot histograms\nplt.figure(figsize=(12, 5))\n\n# Control group plot\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, color='lightblue', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', label=f'Mean = {mean_control:.2f}')\nplt.title(\"Control Group Donations (amount &gt; 0)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\n# Treatment group plot\nplt.subplot(1, 2, 2)\nplt.hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\nplt.axvline(mean_treatment, color='red', linestyle='--', label=f'Mean = {mean_treatment:.2f}')\nplt.title(\"Treatment Group Donations (amount &gt; 0)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\n# Layout adjustment\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nVisually, both groups have similar distributions with right-skewed tails, and their averages are close:\nControl group: $45.54\nTreatment group: $43.87\nThis reinforces the earlier regression result: while the treatment may affect whether someone donates, it doesn’t substantially change how much they donate once they do.\n\n\n\nLaw of Large Numbers\n\ntodo:\nMake a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means._\n\nnp.random.seed(42)\n\n# Draw 10,000 samples with replacement from each group\ntreatment_sample = np.random.choice(treatment_donors, size=10000, replace=True)\ncontrol_sample = np.random.choice(control_donors, size=10000, replace=True)\n\n# Compute differences\ndifferences = treatment_sample - control_sample\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Compute true difference in means from data\ntrue_diff = treatment_donors.mean() - control_donors.mean()\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, color='orange', label='Cumulative Average of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.2f}')\nplt.title(\"Convergence of Simulated Differences in Means\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot illustrates how the cumulative average of simulated differences in donation amounts between the treatment and control groups evolves as the number of simulations increases. Initially, the cumulative average fluctuates due to random variation in the early samples—this is evident in the wavy behavior of the orange line at the start of the graph. However, as more and more random samples are drawn, the cumulative average stabilizes and converges toward the red dashed line, which represents the true difference in mean donation amounts from the full dataset.\nThe cumulative average of the simulated differences steadily approaches the true difference in means, demonstrating the law of large numbers: as the number of independent samples increases, the sample average gets closer to the population average. This convergence reinforces the idea that simulation is a powerful tool in statistical inference. In this case, it confirms that we can reliably estimate the true difference in means between groups through repeated random sampling. The alignment of the cumulative average with the true difference highlights how simulations not only support theoretical principles but also provide intuitive, visual insight into the behavior of sample statistics.\n\n\n\nCentral Limit Theorem\n\ntodo:\nMake 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”_\n\n# Function to simulate differences in means\ndef simulate_differences(n, reps=1000):\n    np.random.seed(42)\n    diffs = []\n    for _ in range(reps):\n        sample_control = np.random.choice(control_donors, size=n, replace=True)\n        sample_treatment = np.random.choice(treatment_donors, size=n, replace=True)\n        diffs.append(sample_treatment.mean() - sample_control.mean())\n    return np.array(diffs)\n\n# Sample sizes to analyze\nsample_sizes = [50, 200, 500, 1000]\n\n# Run simulations for each sample size\nresults = {n: simulate_differences(n) for n in sample_sizes}\n\n# Plot histograms\nplt.figure(figsize=(14, 10))\nfor i, n in enumerate(sample_sizes, 1):\n    plt.subplot(2, 2, i)\n    plt.hist(results[n], bins=30, color='lightgray', edgecolor='black')\n    plt.axvline(0, color='red', linestyle='--', label='Zero')\n    plt.title(f\"Sample Size {n} (1000 Simulations)\")\n    plt.xlabel(\"Average Treatment - Control\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms visualize the sampling distribution of the average treatment effect at different sample sizes: 50, 200, 500, and 1000. Each distribution represents the results of 1,000 simulations in which we repeatedly sampled from the treatment and control groups (among donors) and computed the difference in average donation amounts.\n\n\nInterpretation by Sample Size:\nSample Size 50: The distribution is wide and highly variable, reflecting the imprecision of small samples. Zero is well within the middle of the distribution, suggesting we would often estimate no effect just by chance. This makes it difficult to detect a true difference in means with confidence.\nSample Size 200: The distribution narrows slightly but still includes zero in the central region. At this size, we gain more precision, but there’s still substantial overlap in possible treatment effects, making it harder to distinguish small differences from noise.\nSample Size 500: The distribution is noticeably tighter, and zero begins to drift toward the edge of the histogram. This suggests that we’re beginning to detect the treatment effect more reliably, as most simulated differences are now negative (consistent with the observed data).\nSample Size 1000: The distribution is tightly centered around a negative value, and zero is now clearly in the tail of the distribution. This indicates that with a large enough sample size, the estimated difference in donation amounts becomes precise enough that we can confidently reject the hypothesis of no effect (i.e., zero is unlikely to be the true difference).\n\n\nConclusion:\nAs the sample size increases, the sampling distributions become narrower, and zero moves from the center to the tail. This means that with larger samples, we are more confident that the treatment and control groups have different average donation amounts — even if the effect is relatively small. This visually reinforces the importance of large samples in achieving statistical significance and reliable inference."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html",
    "href": "blog/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007.\nWithin the treatment group, additional variations were introduced: the match ratio was either $1:$1, $2:$1, or $3:$1; the maximum matching amount was set at $25,000, $50,000, $100,000, or left unspecified; and the suggested donation amount was personalized based on each recipient’s past giving history—equal to, 1.25 times, or 1.5 times their highest previous donation. The results showed that including a matching grant offer significantly increased both the likelihood of donation and the average amount donated. However, higher match ratios did not lead to higher giving, contradicting common fundraising assumptions. The impact of the matching grant was especially strong in states that voted Republican in the 2004 presidential election, suggesting that political context played a role in donor responsiveness.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#introduction",
    "href": "blog/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007.\nWithin the treatment group, additional variations were introduced: the match ratio was either $1:$1, $2:$1, or $3:$1; the maximum matching amount was set at $25,000, $50,000, $100,000, or left unspecified; and the suggested donation amount was personalized based on each recipient’s past giving history—equal to, 1.25 times, or 1.5 times their highest previous donation. The results showed that including a matching grant offer significantly increased both the likelihood of donation and the average amount donated. However, higher match ratios did not lead to higher giving, contradicting common fundraising assumptions. The impact of the matching grant was especially strong in states that voted Republican in the 2004 presidential election, suggesting that political context played a role in donor responsiveness.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#data",
    "href": "blog/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset includes information on 50,083 individuals who had previously donated to a nonprofit organization.\nThese individuals were randomly assigned to receive one of several fundraising mailers as part of a natural field experiment testing whether matching donations influence the likelihood and amount of giving.\n\nimport pandas as pd\n\nkarlan_list_2007 = pd.read_stata(\"~/Desktop/MSBA/SP/MGTA495b/hw1/karlan_list_2007.dta\")\nkarlan_list_2007.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nKey variables in the dataset include whether the individual received a treatment letter (treatment), the size of the matching ratio (ratio), and donation outcomes (gave, amount). The dataset also contains demographic and behavioral variables such as prior donation frequency (freq), donation recency (mrm2), and location-level characteristics (e.g., red0, pwhite).\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo test the integrity of the randomization, I compare pre-treatment characteristics such as female, couple, years, freq, and ave_hh_sz between the treatment and control groups. These variables were chosen because they capture demographic traits and prior donation behavior that could influence giving. For example, female captures gender, which may affect responsiveness to messaging; couple accounts for household structure; years and freq reflect donor loyalty and engagement; and ave_hh_sz serves as a proxy for local socioeconomic context. Ensuring these variables are balanced helps validate the random assignment.\n\nimport numpy as np\nfrom scipy import stats\n\nselected_vars = ['female', 'couple', 'years', 'freq', 'ave_hh_sz']\n\ndef manual_t_test(var):\n    data = karlan_list_2007[[var, 'treatment']].dropna()\n    group_A = data[data['treatment'] == 1][var]\n    group_B = data[data['treatment'] == 0][var]\n    X_A, X_B = group_A.mean(), group_B.mean()\n    S_A2, S_B2 = group_A.var(ddof=1), group_B.var(ddof=1)\n    N_A, N_B = len(group_A), len(group_B)\n    se = np.sqrt(S_A2 / N_A + S_B2 / N_B)\n    t_stat = (X_A - X_B) / se\n    df_approx = min(N_A, N_B) - 1\n    p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df_approx))\n    return {\n        \"variable\": var,\n        \"mean_treatment\": X_A,\n        \"mean_control\": X_B,\n        \"t_stat\": t_stat,\n        \"p_value\": p_value,\n        \"significant_95\": p_value &lt; 0.05\n    }\n\nmanual_t_results = [manual_t_test(var) for var in selected_vars]\nmanual_t_df = pd.DataFrame(manual_t_results)\nmanual_t_df\n\n\n\n\n\n\n\n\nvariable\nmean_treatment\nmean_control\nt_stat\np_value\nsignificant_95\n\n\n\n\n0\nfemale\n0.275151\n0.282698\n-1.753513\n0.079533\nFalse\n\n\n1\ncouple\n0.091358\n0.092975\n-0.582258\n0.560401\nFalse\n\n\n2\nyears\n6.078365\n6.135914\n-1.090918\n0.275325\nFalse\n\n\n3\nfreq\n8.035364\n8.047342\n-0.110845\n0.911741\nFalse\n\n\n4\nave_hh_sz\n2.430015\n2.427002\n0.823351\n0.410321\nFalse\n\n\n\n\n\n\n\nThe results of the t-tests for five pre-treatment variables indicate that there are no statistically significant differences between the treatment and control groups at the 95% confidence level. This suggests that the randomization process was successful in creating comparable groups across these covariates. These results match Table 1 of Karlan and List (2007), indicating that the treatment was successfully randomized and there are no meaningful pre-treatment differences.\n\nimport statsmodels.api as sm\nseparate_regression_results = []\n\nfor var in selected_vars:\n    X_var = sm.add_constant(karlan_list_2007[[var]])\n    y = karlan_list_2007['treatment']\n\n    model_var = sm.OLS(y, X_var, missing='drop').fit()\n    \n    separate_regression_results.append({\n        \"variable\": var,\n        \"coef\": model_var.params[var],\n        \"p_value\": model_var.pvalues[var],\n        \"r_squared\": model_var.rsquared\n    })\n\nseparate_regression_df = pd.DataFrame(separate_regression_results)\n\nprint(separate_regression_df)\n\n    variable      coef   p_value     r_squared\n0     female -0.008366  0.078691  6.313390e-05\n1     couple -0.004308  0.559365  6.964784e-06\n2      years -0.000422  0.270016  2.429441e-05\n3       freq -0.000020  0.911702  2.455472e-07\n4  ave_hh_sz  0.004678  0.409801  1.408946e-05\n\n\nUsing bivariate regressions helps me to further confirm that none of the selected covariates are statistically significantly different at the 5% level."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#experimental-results",
    "href": "blog/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nA barplot comparing the response rates between groups shows that the treatment group had a higher donation rate than the control group.\n\nimport matplotlib.pyplot as plt\n\ntreatment_proportion = karlan_list_2007[karlan_list_2007['treatment'] == 1]['gave'].mean()\ncontrol_proportion = karlan_list_2007[karlan_list_2007['treatment'] == 0]['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['blue', 'orange'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\nplt.ylim(0, 0.025)\nplt.show()\n\n\n\n\n\n\n\n\nA Welch’s t-test confirms that the difference is statistically significant (p &lt; 0.01), and a bivariate linear regression of gave ~ treatment yields a positive and significant coefficient. These results are consistent with the findings reported in Table 2A, Panel A.\n\ngave_treatment = karlan_list_2007[karlan_list_2007['treatment'] == 1]['gave']\ngave_control = karlan_list_2007[karlan_list_2007['treatment'] == 0]['gave']\n\nresult = manual_t_test('gave')\nresult[\"coef\"] = result[\"mean_treatment\"] - result[\"mean_control\"]\nprint(pd.DataFrame([result]))\n\nX = sm.add_constant(karlan_list_2007['treatment'])  \ny = karlan_list_2007['gave']\n\nmodel = sm.OLS(y, X, missing='drop').fit()\nprint(model.summary())\n\n  variable  mean_treatment  mean_control    t_stat   p_value  significant_95  \\\n0     gave        0.022039      0.017858  3.209462  0.001332            True   \n\n      coef  \n0  0.00418  \n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 07 May 2025   Prob (F-statistic):            0.00193\nTime:                        23:50:44   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nA probit regression, corresponding to Table 3, Column 1 of the original study, confirms that assignment to the treatment group significantly increases the likelihood of donating. The treatment coefficient is approximately 0.325 and is statistically significant at the 1% level.\n\nfrom statsmodels.discrete.discrete_model import Probit\nimport statsmodels.formula.api as smf\n\nprobit_model = smf.probit('gave ~ treatment', data=karlan_list_2007).fit()\n\nmarginal_effects = probit_model.get_margeff(at='overall')\nprint(marginal_effects.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThese results indicate a statistically significant difference in donation behavior between the treatment and control groups. Specifically, individuals who received the treatment—meaning they were told about the matching grant—were significantly more likely to donate compared to those in the control group, who received no such incentive.\nThis finding suggests that even a simple message about a matching donation can meaningfully increase the likelihood of giving. It highlights an important aspect of human behavior: people are more likely to contribute to a cause when they believe their donation will have amplified impact. The idea that someone else is willing to “match” their gift seems to motivate action—perhaps because it signals legitimacy, urgency, or enhanced value of their contribution.\nIn practical terms, this supports the effectiveness of matching grants as a behavioral nudge in charitable fundraising. Organizations seeking to increase donor participation can likely benefit from this strategy, as it taps into psychological mechanisms like reciprocity, perceived value, and social proof.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndef test_match_ratio_effect(group_var_1, group_var_2, label1, label2):\n\n    subset = karlan_list_2007[(karlan_list_2007[group_var_1] == 1) | \n                              (karlan_list_2007[group_var_2] == 1)].copy()\n    \n    subset['treatment'] = subset[group_var_1]  # 1 if group_var_1, 0 if group_var_2\n\n    data_backup = karlan_list_2007.copy() \n    globals()['karlan_list_2007'] = subset  \n\n    result = manual_t_test('gave')\n    result['comparison'] = f\"{label1} vs {label2}\"\n\n    globals()['karlan_list_2007'] = data_backup \n    return result\n\nresults = [\n    test_match_ratio_effect('ratio2', 'ratio', \"2:1\", \"1:1\"),\n    test_match_ratio_effect('ratio3', 'ratio', \"3:1\", \"1:1\"),\n    test_match_ratio_effect('ratio3', 'ratio2', \"3:1\", \"2:1\")\n]\n\nmatch_ratio_effects_df = pd.DataFrame(results)\nprint(match_ratio_effects_df[['comparison', 'mean_treatment', 'mean_control', 't_stat', 'p_value', 'significant_95']])\n\n   comparison  mean_treatment  mean_control    t_stat   p_value  \\\n0  2:1 vs 1:1        0.022633      0.020749  0.965049  0.334541   \n1  3:1 vs 1:1        0.022733      0.020749  1.015017  0.310120   \n2  3:1 vs 2:1        0.022733      0.022633  0.050116  0.960031   \n\n   significant_95  \n0           False  \n1           False  \n2           False  \n\n\nI use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not.The results align with the authors’ comment on page 8 of the paper, which states that neither the match threshold nor the match ratio appears to have a meaningful influence on donor behavior. Specifically, my analysis shows that the differences in donation rates between the 1:1, 2:1, and 3:1 match conditions are small and statistically insignificant. The p-values for all pairwise comparisons are well above the conventional 0.05 threshold, and the t-statistics are close to zero, indicating no meaningful difference in the likelihood of donating across these conditions. This supports the paper’s finding that while offering a match increases giving relative to no match, increasing the match ratio beyond 1:1 does not lead to additional gains. My findings reinforce the idea that it is the presence of a match—rather than its size—that primarily drives donor behavior.\n\nmatched_df = karlan_list_2007[\n    (karlan_list_2007['treatment'] == 1) &\n    (karlan_list_2007['ratio'].isin([1, 2, 3])) &\n    (karlan_list_2007['gave'].notnull())\n].copy()\n\nmatched_df['ratio2'] = (matched_df['ratio'] == 2).astype(int)\nmatched_df['ratio3'] = (matched_df['ratio'] == 3).astype(int)\n\nX = sm.add_constant(matched_df[['ratio2', 'ratio3']])\ny = matched_df['gave']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 07 May 2025   Prob (F-statistic):              0.524\nTime:                        23:50:44   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAssessing the same issue using a regression, the coefficients are not statistically different from each other. The coefficients on the dummy variables for the match ratios represent the difference in the probability of donating relative to the 1:1 match group, which serves as the omitted reference category. These coefficients are small and statistically insignificant, indicating that higher match ratios do not significantly influence the likelihood of making a donation. This result is consistent with the earlier t-test findings and reinforces the conclusion from the original paper that simply offering a match matters more than increasing its size.\n\nkarlan_list_2007['ratio1'] = (karlan_list_2007['ratio'] == 1).astype(int)\nmodel = smf.ols('gave ~ ratio1 + ratio2 + ratio3', data=karlan_list_2007).fit()\n\nresponse_1to1 = karlan_list_2007[karlan_list_2007['ratio1'] == 1]['gave'].mean()\nresponse_2to1 = karlan_list_2007[karlan_list_2007['ratio2'] == 1]['gave'].mean()\nresponse_3to1 = karlan_list_2007[karlan_list_2007['ratio3'] == 1]['gave'].mean()\n\ndiff_2v1_actual = response_2to1 - response_1to1\ndiff_3v2_actual = response_3to1 - response_2to1\n\ncoef_ratio1 = model.params['ratio1']\ncoef_ratio2 = model.params['ratio2']\ncoef_ratio3 = model.params['ratio3']\n\ndiff_2v1_reg = coef_ratio2 - coef_ratio1\ndiff_3v2_reg = coef_ratio3 - coef_ratio2\n\nprint(\"Actual Response Rate Differences:\")\nprint(f\"2:1 vs 1:1 = {diff_2v1_actual:.6f}\")\nprint(f\"3:1 vs 2:1 = {diff_3v2_actual:.6f}\")\n\nprint(\"\\nFitted Coefficient Differences:\")\nprint(f\"2:1 vs 1:1 = {diff_2v1_reg:.6f}\")\nprint(f\"3:1 vs 2:1 = {diff_3v2_reg:.6f}\")\n\nActual Response Rate Differences:\n2:1 vs 1:1 = 0.001884\n3:1 vs 2:1 = 0.000100\n\nFitted Coefficient Differences:\n2:1 vs 1:1 = 0.001884\n3:1 vs 2:1 = 0.000100\n\n\nI also calculated the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Both the actual response rate differences and the fitted regression coefficients show that increasing the match ratio from 1:1 to 2:1 leads to only a small increase in the likelihood of donating, while the jump from 2:1 to 3:1 has virtually no additional effect. This suggests that higher match ratios do not meaningfully boost donation rates beyond what a basic 1:1 match achieves. The findings support the conclusion that it is the presence of a match—not its size—that drives donor behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nresult = manual_t_test(\"amount\")\nprint(pd.DataFrame([result]))\n\n  variable  mean_treatment  mean_control    t_stat   p_value  significant_95\n0   amount        0.966873      0.813268  1.918228  0.055099           False\n\n\nI calculated a t-test of the donation amount on the treatment status, and it shows that the average donation amount was higher in the treatment group, but the difference is not quite statistically significant at the 5% level. The p-value of 0.055 is just above the 0.05 threshold, suggesting that the evidence for a treatment effect on donation amount is suggestive, but not conclusive.\nThis tells us that being offered a match may increase donation amounts, but the effect is relatively modest and not strong enough to be certain under typical statistical standards.\nThen, I limited the data to just people who made a donation and repeat the previous analysis and run a regression which allows me to analyze how much respondents donate conditional on donating some positive amount.\n\ndonors_only = karlan_list_2007[karlan_list_2007['amount'] &gt; 0]\ndonors_model = smf.ols('amount ~ treatment', data=donors_only).fit()\nprint(donors_model.summary2().tables[1])\n\n               Coef.  Std.Err.          t         P&gt;|t|     [0.025     0.975]\nIntercept  45.540268  2.423378  18.792063  5.473578e-68  40.784958  50.295579\ntreatment  -1.668393  2.872384  -0.580839  5.614756e-01  -7.304773   3.967986\n\n\nThis analysis shows that, conditional on donating, individuals in the treatment group did not give more than those in the control group — in fact, they gave slightly less, though the result is not statistically meaningful. In short: while the treatment may increase the likelihood of giving, it does not significantly affect how much is given once someone decides to donate. It seems that the treatment coefficient does not have a causal interpretation in this context. The sample is now restricted to those who chose to give, which introduces selection bias. This means we are no longer comparing randomly assigned groups, and the treatment effect can’t be causally interpreted. This regression only tells us something about donor behavior, not about the overall treatment effect on donation amount.\nNext I made two plots to show the distribution of the donation amounts for donors in treatment and control groups.\n\ntreatment_donors = donors_only[donors_only['treatment'] == 1]['amount']\ncontrol_donors = donors_only[donors_only['treatment'] == 0]['amount']\n\nmean_treatment = treatment_donors.mean()\nmean_control = control_donors.mean()\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, color='lightblue', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', label=f'Mean = {mean_control:.2f}')\nplt.title(\"Control Group Donations (amount &gt; 0)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\nplt.axvline(mean_treatment, color='red', linestyle='--', label=f'Mean = {mean_treatment:.2f}')\nplt.title(\"Treatment Group Donations (amount &gt; 0)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nVisually, both groups have similar distributions with right-skewed tails, and their averages are close:\nControl group: $45.54\nTreatment group: $43.87\nThis reinforces the earlier regression result: while the treatment may affect whether someone donates, it doesn’t substantially change how much they donate once they do."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#simulation-experiment",
    "href": "blog/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nHere I simulated 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. Then I calculated a vector of 10,000 differences, and plotted the cumulative average of that vector of differences.\n\nnp.random.seed(42)\n\ntreatment_sample = np.random.choice(treatment_donors, size=10000, replace=True)\ncontrol_sample = np.random.choice(control_donors, size=10000, replace=True)\n\ndifferences = treatment_sample - control_sample\n\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\ntrue_diff = treatment_donors.mean() - control_donors.mean()\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, color='orange', label='Cumulative Average of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.2f}')\nplt.title(\"Convergence of Simulated Differences in Means\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot illustrates how the cumulative average of simulated differences in donation amounts between the treatment and control groups evolves as the number of simulations increases. Initially, the cumulative average fluctuates due to random variation in the early samples—this is evident in the wavy behavior of the orange line at the start of the graph. However, as more and more random samples are drawn, the cumulative average stabilizes and converges toward the red dashed line, which represents the true difference in mean donation amounts from the full dataset.\nThe cumulative average of the simulated differences steadily approaches the true difference in means, demonstrating the law of large numbers: as the number of independent samples increases, the sample average gets closer to the population average.\n\n\nCentral Limit Theorem\nTo demonstrate the Central Limit Theorem, I generated four histograms using sample sizes of 50, 200, 500, and 1000. For each sample size—for example, 50—I randomly selected 50 observations from both the control and treatment distributions, computed the average difference between them, and repeated this procedure 1,000 times to produce a distribution of average differences.\n\ndef simulate_differences(n, reps=1000):\n    np.random.seed(42)\n    diffs = []\n    for _ in range(reps):\n        sample_control = np.random.choice(control_donors, size=n, replace=True)\n        sample_treatment = np.random.choice(treatment_donors, size=n, replace=True)\n        diffs.append(sample_treatment.mean() - sample_control.mean())\n    return np.array(diffs)\n\nsample_sizes = [50, 200, 500, 1000]\n\nresults = {n: simulate_differences(n) for n in sample_sizes}\n\nplt.figure(figsize=(14, 10))\nfor i, n in enumerate(sample_sizes, 1):\n    plt.subplot(2, 2, i)\n    plt.hist(results[n], bins=30, color='lightgray', edgecolor='black')\n    plt.axvline(0, color='red', linestyle='--', label='Zero')\n    plt.title(f\"Sample Size {n} (1000 Simulations)\")\n    plt.xlabel(\"Average Treatment - Control\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms show the sampling distribution of the average treatment effect at sample sizes of 50, 200, 500, and 1000, based on 1,000 simulations using donor data. Each plot illustrates how the estimated difference in average donation amounts varies with sample size.\n\nn = 50: The distribution is wide and highly variable, with zero near the center—making it hard to detect a true effect.\nn = 200: The distribution narrows, but zero remains near the middle, indicating limited precision.\nn = 500: The spread tightens further, and zero begins to shift toward the edge, suggesting emerging evidence of an effect.\nn = 1000: The distribution is tightly centered around a negative value, with zero clearly in the tail, signaling a detectable difference in means.\n\nConclusion: As sample size increases, the distributions narrow and zero shifts from the center to the tail, highlighting how larger samples enhance precision and confidence in identifying true effects."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html",
    "href": "blog/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\ndf1 = pd.read_csv(\"~/Desktop/MSBA/SP/MGTA495b/hw2/blueprinty.csv\")\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\ncustomers = df1[df1['iscustomer'] == 1]\nnon_customers = df1[df1['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=10, alpha=0.5, label='Customers', color='blue')\nplt.hist(non_customers['patents'], bins=10, alpha=0.5, label='Non-Customers', color='orange')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents by Customer Status')\nplt.legend()\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f\"Mean number of patents for customers: {mean_customers}\")\nprint(f\"Mean number of patents for non-customers: {mean_non_customers}\")\n\n\n\n\n\n\n\n\nMean number of patents for customers: 4.133056133056133\nMean number of patents for non-customers: 3.4730127576054954\n\n\nBy comparing histograms and means of number of patents by customer status, I found that overall, firms using the software tend to hold more patents, and the distribution for customers shows a longer tail, suggesting a greater presence of firms with higher patent activity. While there is considerable overlap between the two groups, the average number of patents is higher for customers, providing preliminary evidence that firms using Blueprinty may be more innovative or productive in securing patents. This supports the marketing team’s claim, though more rigorous analysis would be needed to establish causality.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Bar plot for region distribution by customer status\nregion_counts = df1.groupby(['region', 'iscustomer']).size().unstack()\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Region Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(['Non-Customers', 'Customers'])\nplt.show()\n\n# Box plot for age distribution by customer status\nplt.figure(figsize=(10, 6))\ndf1.boxplot(column='age', by='iscustomer', grid=False)\nplt.title('Age Distribution by Customer Status')\nplt.suptitle('')  \nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Age')\nplt.show()\n\nmean_age_customers = customers['age'].mean()\nmean_age_non_customers = non_customers['age'].mean()\n\nprint(f\"Mean age of patents for customers: {mean_age_customers}\")\nprint(f\"Mean age of patents for non-customers: {mean_age_non_customers}\")\n\n\n\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nMean age of patents for customers: 26.9002079002079\nMean age of patents for non-customers: 26.101570166830225\n\n\nThen, I compared regions and ages by customer status. The comparisons show notable differences in region but minimal differences in age between customers and non-customers. Customers are more concentrated in the Northeast, while non-customers dominate in regions like the Midwest and Southwest, suggesting geography may influence adoption of Blueprinty’s software. In contrast, the age distributions are quite similar, with nearly identical medians and overlapping ranges. The slight difference in mean age is not substantial. Overall, region appears to be more strongly associated with customer status than age.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we use a Poisson distribution to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFor independent observations ( Y_1, Y_2, , Y_n () ), the likelihood function is:\n[ L(Y) = _{i=1}^n = ]\nTaking the natural logarithm of the likelihood, we obtain the log-likelihood:\n[ (Y) = L(Y) = -n+ ( Y_i ) - (Y_i!) ]\nThis is how to code in Python.\n\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  \n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return ll\n\n\n# Example: maximize using scipy\nY = df1['patents']\n\nlambda_values = np.linspace(0.1, 10, 100)\n\nY = df1['patents'].values  # Use the patents data from df1\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow, I used the log-likelihood function to create a plot with lambda values on the horizontal axis and the corresponding log-likelihood values on the vertical axis, using the observed number of patents as input for 𝑌.\nIf desired, we can take the first derivative of the log-likelihood, set it equal to zero, and solve for ( ). This yields the MLE ( = {Y} ), which aligns with our intuition since the mean of a Poisson distribution is ( ). The steps are shown below:\nTaking the derivative with respect to ( ):\n[ = -n + ]\nSet the derivative equal to zero:\n[ -n + = 0 ]\nSolve for ( ):\n[ = = {Y} ]\nFollowing are how to code this in python.\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lambda_):\n    if lambda_ &lt;= 0:\n        return np.inf  \n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return -ll  # Negative because we minimize\n\n# Find MLE using numerical optimization\nresult = minimize_scalar(neg_log_likelihood, bounds=(0.001, 100), method='bounded')\nlambda_mle = result.x\n\n# Compare with sample mean (Ȳ)\nY_bar = np.mean(Y)\n\n# Print both\nprint(f\"MLE for lambda (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (Ȳ): {Y_bar:.4f}\")\n\nMLE for lambda (via optimization): 3.6847\nSample mean of Y (Ȳ): 3.6847\n\n\nI found the MLE by optimizing the Poisson log-likelihood. The estimated value of 𝜆 MLE exactly matched the sample mean of Y, which aligns with the theoretical result that the MLE for a Poisson model is 𝑌 bar.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe updated log-likelihood function now has an additional argument to take in a covariate matrix X. The parameter of the model also changes from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){lambda &lt;- exp(X %*% beta)  # inverse link: exp(X * beta)\n  ll &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n  return(ll)\n}\nIn python, it looks like:\n\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport math\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta).ravel()\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n    linpred = X.dot(beta)\n    linpred = np.clip(linpred, -100, 100)\n    mu = np.array([math.exp(val) for val in linpred])\n    if np.any(mu &lt;= 0) or np.any(np.isnan(mu)):\n        return -np.inf\n    return np.sum(Y * np.log(mu) - mu - gammaln(Y + 1))\n\n\nimport statsmodels.api as sm \n\ndf1['age_squared'] = df1['age'] ** 2\nregion_dummies = pd.get_dummies(df1['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=df1.index, name='intercept'),\n    df1[['age', 'age_squared', 'iscustomer']],\n    region_dummies], axis=1)\nY = df1['patents'].values\nX_matrix = X.values\n\ndef neg_loglikelihood(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\n# Optimize to find MLE\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_loglikelihood, x0=initial_beta, args=(Y, X_matrix), method='BFGS')\n\n# Extract MLE and Hessian inverse\nbeta_mle = result.x\nhess_inv = result.hess_inv\n# Ensure Hessian inverse is array\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\n# Compute standard errors\nstd_errors = np.sqrt(np.diag(hess_inv))\n\n# Build results table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509991\n0.174645\n\n\nage\n0.148706\n0.013491\n\n\nage_squared\n-0.002972\n0.000253\n\n\niscustomer\n0.207609\n0.026491\n\n\nNortheast\n0.029155\n0.042567\n\n\nNorthwest\n-0.017578\n0.054914\n\n\nSouth\n0.056565\n0.026492\n\n\nSouthwest\n0.050567\n0.040233\n\n\n\n\n\n\n\n\n# Fit a Poisson regression model using sm.GLM\n# Fit a Poisson regression model using sm.GLM\nX = X.astype(float)\nY = Y.astype(float)\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display the summary of the results\nprint(poisson_results.summary())\n\n# Compare coefficients from sm.GLM with your results\nglm_coefficients = poisson_results.params\nprint(\"\\nCoefficients from sm.GLM:\")\nprint(glm_coefficients)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        23:50:42   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nintercept      -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\nCoefficients from sm.GLM:\nintercept     -0.508920\nage            0.148619\nage_squared   -0.002970\niscustomer     0.207591\nNortheast      0.029170\nNorthwest     -0.017575\nSouth          0.056561\nSouthwest      0.050576\ndtype: float64\n\n\nThe results show that age is positively associated with the number of patents, suggesting that more established firms tend to be more innovative. However, the negative coefficient on age squared indicates a diminishing return — as firms get older, the increase in patenting tapers off. This non-linear relationship is statistically significant and aligns with the idea that younger firms grow in innovation at first, but eventually plateau.\nThe region coefficients are not statistically significant, meaning that, after controlling for other factors, location does not appear to have a strong influence on patent activity.\nImportantly, the variable iscustomer — indicating whether the firm uses Blueprinty’s software — is positive and highly significant. The coefficient of 0.2080 implies that, all else equal, firms that are customers of Blueprinty tend to have more patent awards. Since the model uses a log link, we can interpret this roughly as a ~23% increase in the expected number of patents (exp(0.208) ≈ 1.231). This supports the claim that firms using Blueprinty’s tools are more successful in securing patents, although causality cannot be established without further analysis.\nTo better understand the effect of Blueprinty’s software on patent success, we simulate two hypothetical scenarios. First, we create two versions of the dataset: one in which no firms use the software (X_0, with iscustomer = 0 for all observations), and another where all firms do (X_1, with iscustomer = 1). Using the fitted Poisson model, we predict the expected number of patents for each firm under both scenarios, generating y_pred_0 and y_pred_1. We then calculate the difference in predictions for each firm and take the average of those differences to estimate the overall effect of using Blueprinty’s software.\n\ndef construct_design_matrix(df1, is_customer=None, region_dummies=None):\n    # Build base matrix\n    X = pd.DataFrame({\n        'Intercept': 1.0,\n        'age': df1['age'].astype(float),\n        'age_squared': df1['age_squared'].astype(float),\n    }, index=df1.index)\n    \n    # Add region dummy variables if provided\n    if region_dummies is not None:\n        region_dummies = region_dummies.astype(float)\n        X = pd.concat([X, region_dummies], axis=1)\n\n    # Add or override iscustomer column\n    if is_customer is None:\n        X['iscustomer'] = df1['iscustomer'].astype(float)\n    else:\n        X['iscustomer'] = float(is_customer)\n\n    # Reorder columns for consistency\n    column_order = ['Intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)\n    return X[column_order]\n\n\n# Build design matrices\nX_full = construct_design_matrix(df1, region_dummies=region_dummies)\nX_0 = construct_design_matrix(df1, is_customer=0, region_dummies=region_dummies)\nX_1 = construct_design_matrix(df1, is_customer=1, region_dummies=region_dummies)\n\n# Define outcome variable\nY = df1['patents'].astype(float)\n\n# Fit Poisson model using GLM\npoisson_model = sm.GLM(Y, X_full, family=sm.families.Poisson())\npoisson_result = poisson_model.fit()\n\n# Predict counterfactual outcomes\ny_pred_0 = poisson_result.predict(X_0)\ny_pred_1 = poisson_result.predict(X_1)\n\n# Calculate average treatment effect\navg_effect = np.mean(y_pred_1 - y_pred_0)\navg_effect\n\n0.7927680710452771\n\n\nOn average, firms that use Blueprinty’s software are predicted to produce about 0.79 more patents than they would if they weren’t customers, assuming all other factors remain the same."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\ndf1 = pd.read_csv(\"~/Desktop/MSBA/SP/MGTA495b/hw2/blueprinty.csv\")\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\ncustomers = df1[df1['iscustomer'] == 1]\nnon_customers = df1[df1['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=10, alpha=0.5, label='Customers', color='blue')\nplt.hist(non_customers['patents'], bins=10, alpha=0.5, label='Non-Customers', color='orange')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents by Customer Status')\nplt.legend()\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f\"Mean number of patents for customers: {mean_customers}\")\nprint(f\"Mean number of patents for non-customers: {mean_non_customers}\")\n\n\n\n\n\n\n\n\nMean number of patents for customers: 4.133056133056133\nMean number of patents for non-customers: 3.4730127576054954\n\n\nBy comparing histograms and means of number of patents by customer status, I found that overall, firms using the software tend to hold more patents, and the distribution for customers shows a longer tail, suggesting a greater presence of firms with higher patent activity. While there is considerable overlap between the two groups, the average number of patents is higher for customers, providing preliminary evidence that firms using Blueprinty may be more innovative or productive in securing patents. This supports the marketing team’s claim, though more rigorous analysis would be needed to establish causality.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Bar plot for region distribution by customer status\nregion_counts = df1.groupby(['region', 'iscustomer']).size().unstack()\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Region Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(['Non-Customers', 'Customers'])\nplt.show()\n\n# Box plot for age distribution by customer status\nplt.figure(figsize=(10, 6))\ndf1.boxplot(column='age', by='iscustomer', grid=False)\nplt.title('Age Distribution by Customer Status')\nplt.suptitle('')  \nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Age')\nplt.show()\n\nmean_age_customers = customers['age'].mean()\nmean_age_non_customers = non_customers['age'].mean()\n\nprint(f\"Mean age of patents for customers: {mean_age_customers}\")\nprint(f\"Mean age of patents for non-customers: {mean_age_non_customers}\")\n\n\n\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nMean age of patents for customers: 26.9002079002079\nMean age of patents for non-customers: 26.101570166830225\n\n\nThen, I compared regions and ages by customer status. The comparisons show notable differences in region but minimal differences in age between customers and non-customers. Customers are more concentrated in the Northeast, while non-customers dominate in regions like the Midwest and Southwest, suggesting geography may influence adoption of Blueprinty’s software. In contrast, the age distributions are quite similar, with nearly identical medians and overlapping ranges. The slight difference in mean age is not substantial. Overall, region appears to be more strongly associated with customer status than age.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we use a Poisson distribution to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFor independent observations ( Y_1, Y_2, , Y_n () ), the likelihood function is:\n[ L(Y) = _{i=1}^n = ]\nTaking the natural logarithm of the likelihood, we obtain the log-likelihood:\n[ (Y) = L(Y) = -n+ ( Y_i ) - (Y_i!) ]\nThis is how to code in Python.\n\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  \n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return ll\n\n\n# Example: maximize using scipy\nY = df1['patents']\n\nlambda_values = np.linspace(0.1, 10, 100)\n\nY = df1['patents'].values  # Use the patents data from df1\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow, I used the log-likelihood function to create a plot with lambda values on the horizontal axis and the corresponding log-likelihood values on the vertical axis, using the observed number of patents as input for 𝑌.\nIf desired, we can take the first derivative of the log-likelihood, set it equal to zero, and solve for ( ). This yields the MLE ( = {Y} ), which aligns with our intuition since the mean of a Poisson distribution is ( ). The steps are shown below:\nTaking the derivative with respect to ( ):\n[ = -n + ]\nSet the derivative equal to zero:\n[ -n + = 0 ]\nSolve for ( ):\n[ = = {Y} ]\nFollowing are how to code this in python.\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lambda_):\n    if lambda_ &lt;= 0:\n        return np.inf  \n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return -ll  # Negative because we minimize\n\n# Find MLE using numerical optimization\nresult = minimize_scalar(neg_log_likelihood, bounds=(0.001, 100), method='bounded')\nlambda_mle = result.x\n\n# Compare with sample mean (Ȳ)\nY_bar = np.mean(Y)\n\n# Print both\nprint(f\"MLE for lambda (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (Ȳ): {Y_bar:.4f}\")\n\nMLE for lambda (via optimization): 3.6847\nSample mean of Y (Ȳ): 3.6847\n\n\nI found the MLE by optimizing the Poisson log-likelihood. The estimated value of 𝜆 MLE exactly matched the sample mean of Y, which aligns with the theoretical result that the MLE for a Poisson model is 𝑌 bar.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe updated log-likelihood function now has an additional argument to take in a covariate matrix X. The parameter of the model also changes from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){lambda &lt;- exp(X %*% beta)  # inverse link: exp(X * beta)\n  ll &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n  return(ll)\n}\nIn python, it looks like:\n\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport math\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta).ravel()\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n    linpred = X.dot(beta)\n    linpred = np.clip(linpred, -100, 100)\n    mu = np.array([math.exp(val) for val in linpred])\n    if np.any(mu &lt;= 0) or np.any(np.isnan(mu)):\n        return -np.inf\n    return np.sum(Y * np.log(mu) - mu - gammaln(Y + 1))\n\n\nimport statsmodels.api as sm \n\ndf1['age_squared'] = df1['age'] ** 2\nregion_dummies = pd.get_dummies(df1['region'], drop_first=True)\nX = pd.concat([pd.Series(1, index=df1.index, name='intercept'),\n    df1[['age', 'age_squared', 'iscustomer']],\n    region_dummies], axis=1)\nY = df1['patents'].values\nX_matrix = X.values\n\ndef neg_loglikelihood(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\n# Optimize to find MLE\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_loglikelihood, x0=initial_beta, args=(Y, X_matrix), method='BFGS')\n\n# Extract MLE and Hessian inverse\nbeta_mle = result.x\nhess_inv = result.hess_inv\n# Ensure Hessian inverse is array\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\n# Compute standard errors\nstd_errors = np.sqrt(np.diag(hess_inv))\n\n# Build results table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509991\n0.174645\n\n\nage\n0.148706\n0.013491\n\n\nage_squared\n-0.002972\n0.000253\n\n\niscustomer\n0.207609\n0.026491\n\n\nNortheast\n0.029155\n0.042567\n\n\nNorthwest\n-0.017578\n0.054914\n\n\nSouth\n0.056565\n0.026492\n\n\nSouthwest\n0.050567\n0.040233\n\n\n\n\n\n\n\n\n# Fit a Poisson regression model using sm.GLM\n# Fit a Poisson regression model using sm.GLM\nX = X.astype(float)\nY = Y.astype(float)\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display the summary of the results\nprint(poisson_results.summary())\n\n# Compare coefficients from sm.GLM with your results\nglm_coefficients = poisson_results.params\nprint(\"\\nCoefficients from sm.GLM:\")\nprint(glm_coefficients)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        23:50:42   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nintercept      -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n\nCoefficients from sm.GLM:\nintercept     -0.508920\nage            0.148619\nage_squared   -0.002970\niscustomer     0.207591\nNortheast      0.029170\nNorthwest     -0.017575\nSouth          0.056561\nSouthwest      0.050576\ndtype: float64\n\n\nThe results show that age is positively associated with the number of patents, suggesting that more established firms tend to be more innovative. However, the negative coefficient on age squared indicates a diminishing return — as firms get older, the increase in patenting tapers off. This non-linear relationship is statistically significant and aligns with the idea that younger firms grow in innovation at first, but eventually plateau.\nThe region coefficients are not statistically significant, meaning that, after controlling for other factors, location does not appear to have a strong influence on patent activity.\nImportantly, the variable iscustomer — indicating whether the firm uses Blueprinty’s software — is positive and highly significant. The coefficient of 0.2080 implies that, all else equal, firms that are customers of Blueprinty tend to have more patent awards. Since the model uses a log link, we can interpret this roughly as a ~23% increase in the expected number of patents (exp(0.208) ≈ 1.231). This supports the claim that firms using Blueprinty’s tools are more successful in securing patents, although causality cannot be established without further analysis.\nTo better understand the effect of Blueprinty’s software on patent success, we simulate two hypothetical scenarios. First, we create two versions of the dataset: one in which no firms use the software (X_0, with iscustomer = 0 for all observations), and another where all firms do (X_1, with iscustomer = 1). Using the fitted Poisson model, we predict the expected number of patents for each firm under both scenarios, generating y_pred_0 and y_pred_1. We then calculate the difference in predictions for each firm and take the average of those differences to estimate the overall effect of using Blueprinty’s software.\n\ndef construct_design_matrix(df1, is_customer=None, region_dummies=None):\n    # Build base matrix\n    X = pd.DataFrame({\n        'Intercept': 1.0,\n        'age': df1['age'].astype(float),\n        'age_squared': df1['age_squared'].astype(float),\n    }, index=df1.index)\n    \n    # Add region dummy variables if provided\n    if region_dummies is not None:\n        region_dummies = region_dummies.astype(float)\n        X = pd.concat([X, region_dummies], axis=1)\n\n    # Add or override iscustomer column\n    if is_customer is None:\n        X['iscustomer'] = df1['iscustomer'].astype(float)\n    else:\n        X['iscustomer'] = float(is_customer)\n\n    # Reorder columns for consistency\n    column_order = ['Intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)\n    return X[column_order]\n\n\n# Build design matrices\nX_full = construct_design_matrix(df1, region_dummies=region_dummies)\nX_0 = construct_design_matrix(df1, is_customer=0, region_dummies=region_dummies)\nX_1 = construct_design_matrix(df1, is_customer=1, region_dummies=region_dummies)\n\n# Define outcome variable\nY = df1['patents'].astype(float)\n\n# Fit Poisson model using GLM\npoisson_model = sm.GLM(Y, X_full, family=sm.families.Poisson())\npoisson_result = poisson_model.fit()\n\n# Predict counterfactual outcomes\ny_pred_0 = poisson_result.predict(X_0)\ny_pred_1 = poisson_result.predict(X_1)\n\n# Calculate average treatment effect\navg_effect = np.mean(y_pred_1 - y_pred_0)\navg_effect\n\n0.7927680710452771\n\n\nOn average, firms that use Blueprinty’s software are predicted to produce about 0.79 more patents than they would if they weren’t customers, assuming all other factors remain the same."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "href": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\ndf = pd.read_csv(\"~/Desktop/MSBA/SP/MGTA495b/hw2/airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Step 1: Data cleaning\nvars_to_keep = [\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms',\n    'price', 'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\n\n# Ensure the required columns exist in the DataFrame\nmissing_columns = [col for col in vars_to_keep if col not in df.columns]\n\ndf = df[vars_to_keep].dropna()\n\n# Step 2: Feature engineering\ndf['instant_bookable'] = (df['instant_bookable'] == 't').astype(int)\ndf = pd.get_dummies(df, columns=['room_type'], drop_first=True)\n\n# Step 3: Build covariate matrix\nX_cols = [\n    'days', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n] + [col for col in df.columns if col.startswith('room_type_')]\n\nX = sm.add_constant(df[X_cols]).astype(float)\nY = df['number_of_reviews'].astype(int)\n\n# Step 4–6: Fit Poisson regression using built-in GLM\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\n# Step 7: Extract coefficients and standard errors\ncoef_table = pd.DataFrame({\n    'Coefficient': result.params,\n    'Std. Error': result.bse\n}, index=X.columns)\n\nprint(coef_table)\n\n                           Coefficient    Std. Error\nconst                         3.498049  1.609066e-02\ndays                          0.000051  3.909218e-07\nbathrooms                    -0.117704  3.749225e-03\nbedrooms                      0.074087  1.991742e-03\nprice                        -0.000018  8.326458e-06\nreview_scores_cleanliness     0.113139  1.496336e-03\nreview_scores_location       -0.076899  1.608903e-03\nreview_scores_value          -0.091076  1.803855e-03\ninstant_bookable              0.345850  2.890138e-03\nroom_type_Private room       -0.010536  2.738448e-03\nroom_type_Shared room        -0.246337  8.619793e-03\n\n\nIn this analysis, I used a Poisson regression model to examine how various listing characteristics relate to the number of Airbnb reviews, which I treated as a proxy for bookings. I began by cleaning the data, keeping only relevant variables and dropping rows with missing values. I converted the instant_bookable variable into a binary indicator and created dummy variables for room_type, dropping one category to avoid multicollinearity. I then built a covariate matrix that included features such as listing age (days), number of bathrooms and bedrooms, nightly price, several review scores, booking status, and room type. Using statsmodels’ built-in GLM() function with a Poisson family, I fit the model and extracted coefficient estimates and standard errors.\nBelow are interpretations of my result:\n\nIntercept (const): The baseline log expected number of reviews for the reference category (likely an entire home/apt. that’s not instantly bookable, with all other variables = 0) is positive and significant.\ndays: Positively associated with number of reviews — makes sense, as listings that have been active longer tend to accumulate more reviews.\nbathrooms: Slightly negative, suggesting listings with more bathrooms might not get significantly more reviews, possibly due to being more niche or expensive.\nbedrooms: Positive effect — more bedrooms tend to attract more bookings (and thus reviews).\nprice: Essentially zero — price per night doesn’t appear to be strongly associated with review counts in this model.\nReview scores:\n\ncleanliness: Positive — cleaner listings tend to get more reviews.\nlocation and value: Both negatively associated with review count, possibly due to correlation with other variables or nonlinear effects.\n\ninstant_bookable: Strong positive effect — listings that are instantly bookable tend to receive more reviews, supporting the idea that ease of booking increases demand.\nroom_type:\n\nPrivate room: Slight negative effect compared to entire home/apt.\nShared room: Strong negative effect — shared rooms receive significantly fewer reviews than entire homes/apts.\n\n\nThe results suggest that listings available for more days, those that are instantly bookable, and those with more bedrooms tend to receive more reviews. In contrast, listings with shared or private rooms tend to receive fewer reviews compared to entire homes or apartments. Cleanliness scores were positively associated with reviews, while location and value scores showed slight negative associations. Interestingly, the price per night had virtually no effect. Overall, the model highlights how certain listing characteristics—particularly availability, ease of booking, and property type—play a meaningful role in driving guest engagement, as measured by review counts."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jenny Li",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "hw2.html",
    "href": "hw2.html",
    "title": "jensite",
    "section": "",
    "text": "import pandas as pd\n\n\ndf1 = pd.read_csv(\"~/Desktop/MSBA/SP/MGTA495b/hw2/blueprinty.csv\")\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\ncustomers = df1[df1['iscustomer'] == 1]\nnon_customers = df1[df1['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=10, alpha=0.5, label='Customers', color='blue')\nplt.hist(non_customers['patents'], bins=10, alpha=0.5, label='Non-Customers', color='orange')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents by Customer Status')\nplt.legend()\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f\"Mean number of patents for customers: {mean_customers}\")\nprint(f\"Mean number of patents for non-customers: {mean_non_customers}\")\n\n\n\n\n\n\n\n\nMean number of patents for customers: 4.133056133056133\nMean number of patents for non-customers: 3.4730127576054954\n\n\n\n# Bar plot for region distribution by customer status\nregion_counts = df1.groupby(['region', 'iscustomer']).size().unstack()\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Region Distribution by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.legend(['Non-Customers', 'Customers'])\nplt.show()\n\n# Box plot for age distribution by customer status\nplt.figure(figsize=(10, 6))\ndf1.boxplot(column='age', by='iscustomer', grid=False)\nplt.title('Age Distribution by Customer Status')\nplt.suptitle('')  \nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Age')\nplt.show()\n\nmean_age_customers = customers['age'].mean()\nmean_age_non_customers = non_customers['age'].mean()\n\nprint(f\"Mean age of patents for customers: {mean_age_customers}\")\nprint(f\"Mean age of patents for non-customers: {mean_age_non_customers}\")\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nMean age of patents for customers: 26.9002079002079\nMean age of patents for non-customers: 26.101570166830225\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\n# Log-likelihood function for Poisson model\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  # return large negative value for invalid lambda\n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return ll\n\n# Example: maximize using scipy\nY = np.array([2, 3, 1, 0, 4])  # example data\n\nresult = minimize_scalar(lambda l: -poisson_loglikelihood(l, Y), bounds=(0.001, 10), method='bounded')\nprint(\"Estimated lambda:\", result.x)\n\nEstimated lambda: 2.000000014292129\n\n\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 10, 100)\n\nY = df1['patents'].values  # Use the patents data from df1\n# Compute the log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambda_values]\n\n# Plot the log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood vs Lambda')\nplt.axvline(x=result.x, color='red', linestyle='--', label=f'Estimated Lambda: {result.x:.2f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lambda_):\n    if lambda_ &lt;= 0:\n        return np.inf  \n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return -ll  # Negative because we minimize\n\n# Find MLE using numerical optimization\nresult = minimize_scalar(neg_log_likelihood, bounds=(0.001, 100), method='bounded')\nlambda_mle = result.x\n\n# Compare with sample mean (Ȳ)\nY_bar = np.mean(Y)\n\n# Print both\nprint(f\"MLE for lambda (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (Ȳ): {Y_bar:.4f}\")\n\nMLE for lambda (via optimization): 3.6847\nSample mean of Y (Ȳ): 3.6847\n\n\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lambda_ = np.exp(X @ beta)\n    if np.any(lambda_ &lt;= 0):\n        return np.inf\n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return -ll\n\n\nimport statsmodels.api as sm \nfrom scipy.optimize import minimize\n\n# Ensure age_sq exists\ndf1['age_sq'] = df1['age'] ** 2\n\n# Create dummy variables for region (drop first to avoid multicollinearity)\nif 'region' in df1.columns:\n    df1 = pd.get_dummies(df1, columns=['region'], drop_first=True)\n\n# Define covariate columns in correct order\nregion_dummies = [col for col in df1.columns if col.startswith('region_') and col != 'region_Southwest']\nX_cols = ['age', 'age_sq'] + region_dummies + ['iscustomer']\n\n# Construct X matrix: constant first, then all covariates\nX = df1[X_cols]\nX.insert(0, 'Intercept', 1.0)  # Ensures first column is all 1's\n\n# Convert to float (safe for optimization)\nX = X.astype(float)\n\n# Response variable\nY = df1['patents'].values\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Use BFGS method for optimization\nresult = minimize(poisson_regression_loglikelihood, beta_init, args=(Y, X), method='BFGS')\n\n# Extract results\nbeta_hat = result.x\nhessian = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian))\n\n# Output table\ncoef_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nprint(coef_table)\n\n                  Coefficient  Std. Error\nIntercept                 0.0         1.0\nage                       0.0         1.0\nage_sq                    0.0         1.0\nregion_Northeast          0.0         1.0\nregion_Northwest          0.0         1.0\nregion_South              0.0         1.0\niscustomer                0.0         1.0\n\n\n/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: overflow encountered in exp\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/conda/lib/python3.12/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n/opt/conda/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: overflow encountered in exp\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n\n\n\n# Fit a Poisson regression model using sm.GLM\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display the summary of the results\nprint(poisson_results.summary())\n\n# Compare coefficients from sm.GLM with your results\nglm_coefficients = poisson_results.params\nprint(\"\\nCoefficients from sm.GLM:\")\nprint(glm_coefficients)\n\nprint(\"\\nCoefficients from your optimization:\")\nprint(beta_hat)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Tue, 06 May 2025   Deviance:                       2143.3\nTime:                        15:48:32   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept           -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq              -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\n====================================================================================\n\nCoefficients from sm.GLM:\nIntercept          -0.508920\nage                 0.148619\nage_sq             -0.002970\nregion_Northeast    0.029170\nregion_Northwest   -0.017575\nregion_South        0.056561\nregion_Southwest    0.050576\niscustomer          0.207591\ndtype: float64\n\nCoefficients from your optimization:\n[0. 0. 0. 0. 0. 0. 0. 0.]\n\n\n\ndf = pd.read_csv(\"~/Desktop/MSBA/SP/MGTA495b/hw2/airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Step 1: Data cleaning\nvars_to_keep = [\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms',\n    'price', 'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\ndf = df[vars_to_keep].dropna()\n\n# Step 2: Feature engineering\ndf['instant_bookable'] = (df['instant_bookable'] == 't').astype(int)\ndf = pd.get_dummies(df, columns=['room_type'], drop_first=True)\n\n# Step 3: Build covariate matrix\nX_cols = [\n    'days', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n] + [col for col in df.columns if col.startswith('room_type_')]\n\nX = sm.add_constant(df[X_cols]).astype(float)\nY = df['number_of_reviews'].astype(int).values\n\n# Step 4: Define Poisson log-likelihood\ndef poisson_loglikelihood(beta, Y, X):\n    lambda_ = np.exp(X @ beta)\n    if np.any(lambda_ &lt;= 0):\n        return np.inf\n    ll = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return -ll  # Negative log-likelihood for minimization\n\n# Step 5: Fit model\nbeta_init = np.zeros(X.shape[1])\nresult = minimize(poisson_loglikelihood, beta_init, args=(Y, X), method='BFGS')\n\n# Step 6: Extract results\nbeta_hat = result.x\nhessian = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian))\n\n# Step 7: Display coefficient table\ncoef_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nprint(coef_table)\n\n                           Coefficient  Std. Error\nconst                              0.0         1.0\ndays                               0.0         1.0\nbathrooms                          0.0         1.0\nbedrooms                           0.0         1.0\nprice                              0.0         1.0\nreview_scores_cleanliness          0.0         1.0\nreview_scores_location             0.0         1.0\nreview_scores_value                0.0         1.0\ninstant_bookable                   0.0         1.0\nroom_type_Private room             0.0         1.0\nroom_type_Shared room              0.0         1.0\n\n\n/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: overflow encountered in exp\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/conda/lib/python3.12/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n/opt/conda/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: overflow encountered in exp\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/conda/lib/python3.12/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)"
  }
]